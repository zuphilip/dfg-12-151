## Digitalisierung 

Die Digitalisierung umfasst immer die Herstellung digitaler Images und die Erzeugung von Metadaten, im Falle von Textwerken gegebenenfalls zusätzlich auch die Volltexterfassung und die Erzeugung von Strukturdaten und Markup.
Wenn im Folgenden von Digitalisierung gesprochen wird, so ist der gesamte Arbeitsgang gemeint (Vorbereitung, Digitalisierung im engeren Sinne, Erzeugung von bibliografischen Metadaten, Strukturdaten, Volltexten sowie Langzeitsicherung/digitale Bestandserhaltung).
Auf eine Differenzierung nach Materialarten (u. a. Druckwerke, unikale Dokumente, Objekte) wird in den allgemeinen Kapiteln im Folgenden verzichtet. 


### Bereitstellung der Materialien, konservatorische Prüfung 

Vorbereitende Tätigkeiten werden bei Digitalisierungsprojekten oft unterschätzt und sollten vor einem Projekt genauestens geprüft werden.
Sind die Objekte überhaupt verfügbar? Gibt es möglicherweise konservatorische Bedenken gegenüber der Digitalisierung der Originale? 
Stehen genügend Mitarbeiterinnen und Mitarbeiter zur Aushebung und Bereitstellung der Objekte zur Verfügung? Ist wissenschaftlich bzw. bibliografisch geschultes Personal zur Hand, das Vollständigkeitskontrollen bzw. Kollationierungen vornimmt, sofern dies aus vorhandenen Katalogeinträgen nicht hervorgeht? Bei Textwerken sollte eine Digitalisierung von unvollständigen oder defekten Drucken nach Möglichkeit vermieden und eine Reproduktion eines vollständigen Exemplars angestrebt werden.

Die konservatorische Prüfung kann sehr viel Zeit in Anspruch nehmen, sollte aber zum Schutz der Objekte keinesfalls unterbleiben.
Es wird empfohlen „Checklisten“ für die Prüfung der Digitalisierungsfähigkeit zu verwenden und daraus die Vorgaben für die Digitalisierung zu entwickeln. [15] Stellt die Reproduktion mit den vorhandenen Reproduktionstechniken ein Risiko bzw. eine besondere Belastung für die Originale dar, [16] so sollte sie – sofern möglich – vom bestehenden Mikrofilm [17] erfolgen oder ganz unterbleiben.
In jedem Fall sind wertvolle historische Materialien mit der konservatorisch nötigen Sorgfalt zu behandeln, auch wenn dies den Durchsatz bei der Digitalisierung reduziert und einen erhöhten Zeitaufwand erfordert.
Die Scantechnik und der Einsatz von Hilfsmitteln, wie Buchkeile oder Fixierhilfen, sind entsprechend den Vorgaben der konservatorischen Prüfung zu wählen. 


[15] Kriterien einer konservatorischen Prüfung sollten sein: Druckfarbe/Tinte/Tusche gefährdet; Farbauftrag/Grundierung gefährdet; Tintenfraß/Farbfraß; Heftung lose; Bünde oder Überzugmaterial zu steif/unflexibel; Bünde (an-)gebrochen; Überzug im Gelenk (an-)gebrochen; Rückeneinlage zu steif; Einbandrücken beschädigt; Brüche, Risse oder Fehlstellen; Ledernarben im Rücken empfindlich; Rückenvergoldung gefährdet; sehr dicker Pergamentband mit hohlem Rücken; Buchdeckel (an-)gebrochen; Buchdeckel gelöst (vorne/hinten); Schließenriemen zu steif/angebrochen, mikrobieller Befall; Mikroform vorhanden.
Darüber hinaus bestehen mitunter auch technische Probleme, die eine Digitalisierung erschweren oder verhindern: Bundsteg zu schmal; Buchblock wellig; Blätter/Lagen sehr steif; Textverlust unvermeidbar; Planlage der Vorlage unmöglich; extremes Format.  
[16] Corbach, Almuth: Bestandsschonendes Digitalisieren von schriftlichem Kulturgut. In: Digital und analog. Die beiden Archivwelten. 46. Rheinischer Archivtag. Ratingen 21.-22. Juni 2012. Beiträge. Hrsg.: LVR-Archivberatungs- und Fortbildungszentrum. Bonn 2013 (Archivhefte 43), S. 90-102; 
[17] In Archiven ist die Digitalisierung vom Mikrofilm unabhängig von konservatorischen Erwägungen durchaus Praxis. Zu den materialspezifischen Parametern für eine Digitalisierung vom Mikrofilm vgl. Kapitel 3.2.2. 


### Technische Parameter der digitalen Reproduktion 

Ziel der Digitalisierung ist die möglichst originalgetreue Wiedergabe des Objektes nach Maßgabe der wissenschaftlichen Erfordernisse.
Die anzuwendenden Parameter für die Digitalisierung sind mit Blick auf die Qualität des Bildes, seine Langzeitverfügbarkeit und Interoperabilität zu wählen. 


#### Allgemeine Erläuterungen und Parameter 

Bei der Herstellung sind grundsätzlich zwei Formen von Digitalisaten zu berücksichtigen.
Zum einen der sogenannte digitale Master, also das Ausgangs- oder Archivformat, und für die Nutzung hergestellte Derivate, meist verkleinerte Kopien in anderen Dateiformaten.
Derivate wie z. B. JPEG-Dateien sind in Abhängigkeit von der gewählten Präsentation aus den Mastern zu erzeugen und können nach Bedarf modifiziert werden.
Eine Modifikation wird z. B. nötig, wenn sich die bei den Nutzerinnen und Nutzern zu erwartenden Bildschirmauflösungen ändern oder Bildformate zum Einsatz kommen sollen, die optimierte Eigenschaften für die gewünschte Anzeige haben (z. B. stufenfreies Zoomen; fließende Übergänge zwischen Abschnitten großer, gleichzeitig aber detailreicher Objekte). 

Der Master ist die Grundlage für alle weiteren Prozesse.
Daher sollte man seiner Herstellung besondere Aufmerksamkeit schenken und bei der Langzeitsicherung entsprechende Empfehlungen berücksichtigen. 

Die folgenden Richtlinien für die Mindestanforderungen an die Digitalisierung beziehen sich ausschließlich auf den digitalen Master. 


##### Auflösung 

Das gebräuchliche Maß für das Auflösungsvermögen einer Datei ist dpi (dots per inch), gleich Bildpunkte pro inch.

Als Untergrenze sollte daher eine Scanauflösung für die Digitalisate gewählt werden, bei der die Details einer Vorlage vollständig in einer gleichgroßen Reproduktion wiedergegeben werden können. [18] 

Daher gilt die grundsätzliche Empfehlung von 300 dpi bezogen auf das Vorlagenformat als Zielauflösung (Vorlagenformat = Ausgabeformat des Digitalisats bei 300 dpi). 

300 dpi beziehen sich allerdings nur auf Vorlagen, die für die Betrachtung mit bloßem Auge gedacht sind, wie etwa: Textwerke, Grafiken und fotografische Aufsichtvorlagen.
Anders verhält es sich bei Medien, deren vollständige Bildinformation nur in Vergrößerung erfassbar wird. 
Das sind Miniaturen jeglicher Art und ganz besonders fotografische Durchsichtmedien (z. B. Negative oder Dias). [19]
 
Weiterhin gibt es Objekte, insbesondere Objekte der Architektur, aber auch überformatige Malereien, die sich auf Grund ihrer Dimensionen den oben beschriebenen Verfahren zur Bestimmung der Auflösung entziehen. 



In der bildmäßigen Fotografie für Sammlungsobjekte und Architekturfotografie wird beispielsweise mit einer modernen 50-Megapixel-Digitalkamera eine Auflösung von 300 dpi bei DIN A2 Ausgabegröße erreicht.
Diese Auflösung ist ausreichend für vielseitige Verwendungen wie Druckerzeugnisse unterschiedlicher Größe und die Präsentation im Internet.
Jedoch können höhere Auflösungen notwendig sein, wenn bildwichtige Details mit diesen Vorgaben nicht dargestellt werden können (z. B. bei Reproduktionen von Gemälden größer als doppelt DIN A0). [20] 



Hinweise für eine gegebenenfalls sinnvolle Abweichung von der grundsätzlichen Auflösungsempfehlung von 300 dpi bezogen auf das Vorlagenformat werden bei den materialspezifischen Parametern genannt. (→ 3.2.2) 

Die erreichbare Auflösung eines digitalen Aufnahmesystems ist dabei nicht nur von der Anzahl der zur Verfügung stehenden Pixel einer Scanzeile oder eines Aufnahmesensors abhängig. 
Sie ist ein Zusammenspiel aus unterschiedlichen Faktoren.
So spielen etwa die technische Qualität des digitalen Aufzeichnungsgeräts (Kamera, Scanner) und die Abbildungsleistung der verwendeten Objektive die entscheidende Rolle. 

Ob ein digitales Aufnahmesystem für ein Digitalisierungsvorhaben geeignet ist, lässt sich vorab im Rahmen einer Testdigitalisierung mit genormten Testcharts (ISO-12233-Testchart, USAF-1951-Testchart) ermitteln. 

Die effektive Auflösung der Testbilder lässt sich dann mit einer Analysesoftware oder mit entsprechenden, in der Regel beigefügten Auflösungstabellen ermitteln. 


[18] Ein Ansatz zur Ermittlung der notwendigen Mindestauflösung eines Digitalisats ist das Auflösungsvermögen des menschlichen Auges bei deutlicher Sehweite.
Die kürzeste Sehweite, bei der man einen Gegenstand ermüdungsfrei über einen längeren Zeitraum betrachten kann, liegt bei ca. 25 cm.
Das menschliche Auge ist in der Lage, zwei Linien noch zu unterscheiden, wenn das Licht auf mindestens zwei nicht benachbarte Sehzellen trifft und mindestens eine Sehzelle dazwischen liegt.
Eine Rasterfrequenz von 60 Linien/cm bei 20 cm Betrachtungsabstand kann nicht mehr als getrennte Linien wahrgenommen werden.
Daraus resultiert folgende Auflösungsanforderung, die auch in der Druckindustrie gebräuchlich ist: 
60 Linien/cm (60-er Raster) benötigen mindestens 120 Pixel zur Darstellung. 
120 px/cm x 2,54 cm/inch = 304,8 dpi, rund 300 dpi 
[19] Beispiel: Kleinbildnegative mit der Größe von 24 x 36 mm sind für die anschließende Vergrößerung auf Fotopapier hergestellt worden. 
Sie eignen sich nicht für die Betrachtung mit bloßem Auge. 
Als fotografische Praxisregel gilt, dass sich Kleinbildnegative 
(abhängig von Objektiv, Belichtung, Film und Entwickler) ca. 10-fach vergrößern lassen, also auf einen Fotoabzug der Größe von 
24 x 36 cm. 
Um diese Zielgröße von 24 x 26 cm nach obiger Maßgabe im Digitalisat des KB-Negativs zu erreichen, müsste es mit mindestens 3.000 dpi digitalisiert werden. 
[20] Bei einem Flächensensor mit 5.700 x 8.600 Pixeln ergibt sich also nach der Beziehung Kamerapixel/Zielauflösung in dpi = 
Zielgröße in inch folgende Ausgabegröße bei 300 dpi: 

8600/300 = 18,7 inch x 2,54 cm/inch = 72 cm 

5700/300 = 12,7 inch x 2,54 cm/inch = 48 cm 


##### Farbtiefe 

Die Farbtiefe bestimmt die Differenzierung der Helligkeits- und Farbwerte in einem Digitalisat. 
Da in der digitalen Technik nur diskrete Zustände (ja/nein) möglich sind, können Helligkeits- und Farbunterschiede – im Gegensatz zur analogen Fotografie – nicht kontinuierlich, d. h. mit fließenden Übergängen dargestellt werden. 

* Bei einer Farbtiefe von 1 Bit sind zwei unterschiedliche Zustände möglich: Weiß und Schwarz. 
* Bei einer Farbtiefe von 8 Bit/Farbkanal sind es 2^8 = 256 Helligkeitsstufen/Farbkanal von Weiß nach Schwarz. 
* Bei einer Farbtiefe von 16/Farbkanal Bit sind es 2^16 = 65.536 
Helligkeitsstufen/Farbkanal von Weiß nach Schwarz. 
* Graustufenbilder mit nur einem Kanal für die Helligkeit haben somit eine Farbtiefe von 8 oder 16 Bit. 
* Farbbilder im RGB-Modus (je ein Farbkanal für Rot, Grün und Blau) besitzen somit eine Farbtiefe von 3 x 8 Bit = 24 Bit, bzw. 3 x 16 Bit = 48 Bit. 

Der Vorteil der höheren Farbtiefe von 16 Bit pro Kanal liegt also in der größeren Farbdifferenzierung.
Dadurch gehen bei einer späteren Bildbearbeitung weniger Tonwerte verloren.
Dieser Bearbeitungsspielraum ist wichtig bei bearbeitungsintensiven Digitalisaten, z. B. von SW-Negativen. 

Digitale Aufnahmesysteme zeichnen Farben im RGB-Farbmodus auf.
Die Analog-/Digitalwandler in hochleistungsfähigen chipbasierten Scanbacks und Zeilenscanbacks können Helligkeitsunterschiede mit 16 Bit differenzieren.
Bei den meisten digitalen KB-Kameras und Flachbettscannern sind es 14 Bit. 

Digitale Aufnahmesysteme zeichnen grundsätzlich mit einer hohen Farbtiefe von 14 Bit bzw. 16 Bit pro Kanal auf.
Um die Informationsdichte der erzeugten Bilddateien zu erhalten, wird im Idealfall eine notwendige Nachbearbeitung der Bilddaten in 16 Bit pro Kanal, d. h. 48 Bit durchgeführt, mindestens jedoch in 8 Bit pro Kanal, d. h. 24 Bit Farbtiefe. 

Für die Sicherung des finalen digitalen Master ist eine Farbtiefe von 8 Bit pro Kanal, d. h. 
24 Bit ausreichend, da die heute gängigen Ausgabe- und Anzeigegeräte nur eine Tonwertwiedergabe mit 8-Bit-Differenzierung unterstützen. 


##### Digitaler Aufnahmeablauf 
 
###### Aufnahmetechnik 

In der fotografischen digitalen Aufnahmetechnik unterscheidet man zwischen der Zeilenscantechnik, bei der eine trilineare Scanzeile mit fester Breite (je eine Zeile ist für eine Farbe – Rot, Grün, Blau – empfindlich) eine bestimmte Strecke abtastet.
Flachbettscanner und Zeilenscanrückteile für Fach- oder Mittelformatkameras arbeiten mit dieser Technik.
Hierbei wird für jeden Bildpunkt die Farbinformation physikalisch erzeugt. 

Das zweite gebräuchliche Verfahren ist der Flächensensor, bei dem auf einer definierten Fläche eine bestimmte Anzahl von Fotodioden angebracht ist. 


Zeilenscanner und multishotfähige Flächensensorkameras finden hauptsächlich Anwendung bei filigranen Motiven wie bildhaften Tiefdruckwerken, Kartenwerken oder Textilien.
Zeilenscanner ermöglichen bei Vorlagen bis DIN A0 hohe Auflösungen und extreme Detailgenauigkeit.
Für große zweidimensionale Vorlagen ab doppelt DIN A0, die zur Gewährleistung der Lesbarkeit mit höchster Detailwiedergabe aufgezeichnet werden sollen, sind Zeilensensoren in Verbindung mit einem Kamerasystem vorzuziehen. 

Mit gebräuchlichen Flachbettscannern lassen sich zweidimensionale Medien bis zu einer Vorlagengröße von DIN A3 digitalisieren.
Das Verfahren ist allerdings nicht berührungsfrei, da die Medien mit dem Vorlagenglas des Scanners in Kontakt kommen, sie eignen sich daher nur für bestandserhalterisch unbedenkliche Vorlagen. 


###### Bildrauschen 

Um die Details einer Vorlage möglichst vollständig zu reproduzieren, sollten bei der eingesetzten Kameratechnik die herstellerspezifischen Empfehlungen zur Verminderung von Bildrauschen beachtet werden.
Grundsätzlich gilt dabei die Faustregel: Je geringer die verwendete ISO-Einstellung ist, desto geringer ist das Rauschen. 


###### Objektive 

Weiterhin empfiehlt sich die Verwendung moderner, hochwertiger Objektive, die für das hohe Auflösungspotential digitaler Kameras entwickelt worden sind.
Festbrennweiten sind Zoomobjektiven vorzuziehen, da bei ihnen Abbildungsfehler gezielt korrigiert werden und der „Über-Kopf-Einsatz“ auf Dauer die Mechanik beschädigt.
Zoomobjektive bieten immer nur einen Qualitätskompromiss über den vorhandenen Brennweitenbereich.
Shiftoptiken vermindern Verluste in der Bildqualität, die durch nachträgliches Entzerren stürzender Perspektivlinien entstehen.
Ist der verwendete Abbildungsmaßstab unter 1:10, sollten spezielle Makroobjektive verwendet werden, die für diesen Abbildungsbereich konstruiert wurden.
Abhängig vom verwendeten Kamerasystem können für Reproduktionen in diesem Maßstabsbereich auch hochwertige Vergrößerungsobjektive eingesetzt werden. [21]

[21] Die Abbildungsleistung eines Objektivs lässt sich auch an den dazugehörigen Modulationstransferfunktions-Diagrammen (MTF-Diagrammen) ablesen.
Diese Diagramme werden von diversen Objektivherstellern veröffentlicht.
Die Modulationstransferfunktion beschreibt, wie gut ein Objektiv den Kantenkontrast in einer Vorlage auf das fotografische Abbild übertragen kann.
Sie bietet somit ein Hinweis auf das Auflösungsvermögen einer Optik. 
Die technischen Datenblätter der Objektive geben auch Auskunft über den Helligkeits- und Schärfeabfall zum Bildrand.
Optische Abbildungsfehler machen sich hier besonders bemerkbar.
Zur Reduzierung der Bildfehler sollte ein Objektiv zweimal abgeblendet werden, um hauptsächlich die bessere Abbildungsleistung der Objektivmitte zu nutzen.
Wenn keine große Tiefenschärfe benötigt wird, wie bei Reproduktionen planer Vorlagen, sollte mit dieser optimalen Arbeitsblende fotografiert werden. 


###### Arbeitsplatz 

Zur Vermeidung von Unschärfen durch Verwacklungen während des Digitalisierens sollte darauf geachtet werden, dass bei der Wahl des Standorts für eine Digitalisierungseinheit ein erschütterungsfreier Platz gesucht wird.
Holzfußböden z. B. sind ungeeignet, da sie schwingen und somit Vibrationen leicht übertragen.
Werden Spiegelreflexkameras benutzt, sollten sie mit Spiegelvorauslösung ausgelöst werden, um Vibrationen des Spiegelschlags auszuschließen. 
Stative und Reprosäulen sollten so dimensioniert sein, dass sie auch das Gewicht der eingesetzten Kameras problemlos halten können. 


###### Moiré 

Bei Objekten mit feinsten, gleichmäßig verteilten Details kann das Oneshot-Verfahren bei Digitalkameras mit Flächensensor zu Farbverfälschungen und Moiré-Effekten führen. [22] 

Ein Moiré-Effekt tritt in der digitalen Bilderzeugung auf, wenn in Abhängigkeit von der Pixelgröße der Scanzeile oder des Flächensensors, des Abbildungsmaßstabes und der Linienfrequenz der Vorlage die gleichmäßige Pixelmatrix eines digital erzeugten Bildes mit einer regelmäßigen Linienstruktur der Vorlage interferiert.
Das Auftreten dieses Effekts ist schwer vorhersehbar, da die entsprechenden Parameter selten konstant bleiben. 

Farbverfälschungen treten bei Oneshot-Flächensensoren unter den gleichen Bedingungen auf, sind aber ein Interpolationsfehler der Kamerasoftware.
Bei Oneshot-Kamerasystemen ist ein Pixel jeweils nur für eine der Grundfarben Rot, Grün, Blau empfindlich.
Die restlichen Farbinformationen für den jeweiligen Bildpunkt werden aus den Farbinformationen der umliegenden Pixel interpoliert.
Sind in Pixelabstand die Farb- und Helligkeitsunterschiede der Vorlage zu extrem, kommt es zu Farbverfälschungen.
In solchen Fällen können Zeilenscanner oder multishotfähige Digitalrückteile bessere Ergebnisse liefern, da hier die Farbinformation physikalisch erzeugt wird und nicht rechnerisch.
Im Gegenzug erhöht sich jedoch der Digitalisierungsaufwand etwas, da die Scanzeiten länger sind und eine absolut konstante Beleuchtung während des Aufnahmevorgangs unabdingbar ist. 

Zur Vermeidung des Moiré-Effekts sollte das notwendige Verfahren und die notwendige Auflösung im Vorfeld eines Digitalisierungsvorhabens an Hand einer Testdigitalisierung ermittelt werden. 

Beispielhaft zu nennende Objekte, bei denen Moiré auftreten kann, sind: gedruckte Halbtonvorlagen, Kupferstiche, Textilien.


[22] Das hier behandelte Moiré ist nicht mit dem Phänomen zu verwechseln, das aus der Interferenz von Druckpunkten gerasterter Vorlagen mit den Bildpunkten des Monitors entsteht. 


###### Beleuchtungstechnik 

Die Wahl der richtigen Beleuchtungstechnik ist im Vorfeld eines Digitalisierungsvorhabens unter bestandserhalterischen Gesichtspunkten zu prüfen.
Zeilenscansysteme benötigen flickerfreies Dauerlicht, da hier über einen bestimmten Zeitraum die Vorlage kontinuierlich abgetastet wird.
Lichtleistung und -farbe müssen während des Scanvorgangs konstant sein, um gleichmäßige Ergebnisse zu erzielen.
Flächensensoren können auch mit Blitzlicht benutzt werden.
Bei Multishotsystemen ist darauf zu achten, dass die Blitzleistung und -farbe während der einzelnen Belichtungen konstant bleiben.
Gleiches gilt für Dauerlicht.

Unter konservatorischen Gesichtspunkten lässt sich die Meinung vertreten, dass eine kurzfristige hohe Lichtbelastung weniger schädlich ist als eine hohe Wärmebelastung, zumal bezüglich der Schadwirkung des Lichts die Lichtbilanz zählt, d. h. es ist unerheblich, ob die gleiche Lichtmenge über einen längeren Zeitraum verteilt oder kurzfristig auf das Objekt trifft.
Kurzfristige hohe Helligkeitsunterschiede sind jedoch aus arbeitsmedizinischer Sicht für die Mitarbeitenden problematisch.


###### Bildverarbeitung 

Fotografische Aufnahmen werden grundsätzlich im herstellerbedingten Rohdatenformat (RAW-Format) in der maximalen Größe erzeugt, mit einer Farbtiefe von 14 oder 16 Bit pro Farbkanal.
RAW-Bilder repräsentieren die originalen Kameradaten.
Bildkorrekturen sollten möglichst in RAW-Software vorgenommen werden.
Da diese Datei nicht verändert wird, kann sie bei Bedarf verlustfrei neu korrigiert werden.
Bearbeitungsvorschriften können überdies als Preset auf beliebig viele Bilder angewendet werden, wodurch eine hohe Arbeitseffizienz erreicht wird. 

Als Profil für Farbaufnahmen sind entweder Adobe RGB, der von der European Color Initiative (ECI) [23] empfohlene Arbeitsfarbraum ECI-RGB v2 oder der dazu identische Farbraum L-Star RGB [24] zu wählen.
CMYK-Farbräume kommen als ausschließliche Druckausgabeprofile nicht in  Frage. 

Für eine verlässliche Farbabstimmung in der späteren Bildverarbeitung ist es notwendig, 
Graukeile, Farbkeile oder Farbcharts mit zu produzieren.
Sofern man keine Einstellungsänderung vornimmt, ist es ausreichend, den diese pro Bildserie nur einmal repräsentativ für die ganze Serie aufzunehmen. 

Bei der Helligkeits- und Tonwertsteuerung ist darauf zu achten, dass das Tonwerthistogramm 
(für Schwarz und Weiß) seitlich nicht angeschnitten wird, damit der Tonwertumfang erhalten bleibt.
Nach heutigem Stand sind Tonwerte von 95 % für Schwarz und 5 % für Weiß differenziert druckbar.
Als maximale Helligkeitswerte empfehlen sich: 

Schwarz:  RGB 16/16/16, Graustufen 90 % 

Weiß:  RGB 232/232/232, Graustufen 10 % 

Bei Reproduktionen von zweidimensionalen Vorlagen sind Freisteller so zu gestalten, dass die gesamte Vorlage mit leichtem, umlaufendem Rand abgebildet wird.
Nur so ist erkennbar, dass nichts von der Vorlage abgeschnitten wurde.
Das Nachschärfen („Unscharfmaskieren“) sollte für den digitalen Master nur moderat erfolgen.
Anwendungsgebunden kann für unterschiedliche Derivate eine zusätzliche Schärfung erfolgen. 

Notwendige Bildbearbeitungen in der Bilddatei sind im Idealfall bei 16 Bit Farbtiefe pro Kanal vorzunehmen, mindestens jedoch in 8 Bit pro Kanal, d. h. 24 Bit Farbtiefe.
Sind alle Korrekturen vorgenommen, wird die RAW-Datei anschließend in eine TIFF-Datei mit 24 Bit/RGB oder 8 Bit/Graustufen als Master konvertiert.
Sind noch weitere Bearbeitungen in der TIFF-Datei vorgesehen, kann die Konvertierung aus der RAW-Datei mit 48 Bit/RGB oder 16 Bit/Graustufen erfolgen.
Nach Beendigung aller notwendigen Bearbeitungsschritte wird der finale Master als TIFF mit 24 Bit/RGB oder 8 Bit/Graustufen gesichert und gilt als archivtauglich.
Das RAW-Format ist als proprietäres Dateiformat allerdings nicht archivtauglich. 

Für einen verlässlichen Abgleich originaler Vorlagen mit der Monitordarstellung des Digitalisats sollten die Vorlagen unter D50-Normlichtumgebung für grafische Arbeitsplätze nach ISO 3664:2000 betrachtet werden.
Monitormodelle, die in der grafischen Industrie Verwendung finden, liefern in diesem Zusammenhang die verbindlichsten Darstellungen.
Sie bieten über die gesamte Grauachse eine konsistente Farbwiedergabe.
Die Monitore sollten mit einem Colorimeter auf die entsprechenden Zielwerte für eine D50-Normlichtumgebung kalibriert werden.
Dabei ist zu beachten, dass eine Farbtemperatur von 5.800 K statt 5.000 K am Monitor eingestellt wird, da 5.000 K am Monitor gelber wahrgenommen werden als 5.000 K der D50-Normlichtumgebung.
Folgende Eckwerte sind zu empfehlen: [25] 
 
* Luminanz: mindestens 120 cd/m^2
* Weißpunkt: 5800 K mit chromatischer Adaption 
* Gradation: 1.8 Gamma oder alternativ L* 
* Farbräume: ISOcoated v2 und ECI-RGB 1.0/2.0 



[23] European Color Initiative: http://www.eci.org 
[24] http://www.colormanagement.org/de/workingspaces.html 
[25] Vgl. die Empfehlungen des Schweizer Kompetenzzentrums für Medien- und Drucktechnologie: http://www.ugra.ch 


###### Digitale Bildnachbearbeitung 

Zur Optimierung der Bildqualität digitaler Aufnahmen ist in der Regel eine zusätzliche Nachbearbeitung notwendig.
Dabei sollte sich diese auf notwendige Farb- und Tonwertkorrekturen beschränken.
Auf jeden Fall zu vermeiden sind Objektdeformationen, das Hinzufügen oder Löschen von Objektteilen, sowie Spezialeffekte, wie z. B. der Einsatz von Verfremdungsfiltern.
Zur Steigerung der Bildintegrität können Positionierungshilfen nachträglich aus dem Bild entfernt werden oder Hintergründe bereinigt werden. 


##### Dateiformate 

Bildmaster von Graustufen oder Farbbildern sollten nach dem derzeitigen Kenntnisstand in „TIFF uncompressed“ [26] langzeitgesichert werden.
Das Format TIFF gibt es schon seit den 1980er Jahren.
Es hat sich als einer der wichtigsten De-Facto-Standards etabliert und es ist damit zu rechnen, dass es auch in Zukunft von allen Standardprogrammen unterstützt wird.
Allerdings gilt dies nur für so genannte „Baseline-TIFFs“.
Die weitgehenden Optionen von Extended TIFFs sollten für den digitalen Master nicht genutzt werden. 

Neben TIFF kann auch TIFF-LZW oder JPEG2000 [27] in seiner verlustfreien Form als Format für den Bildmaster verwendet werden.
Für die Speicherung von Mastern im JPEG2000-Format ist allerdings darauf zu achten, dass nur die lizenzfreien Bereiche von JPEG2000 Verwendung finden. 

In den letzten Jahren gelangten gerade angesichts der zunehmenden Speicherproblematik der ISO-Standard JPEG2000 [28] und nach Ablaufen der Lizenzen TIFF-LZW als effiziente Kompressionsformate ins Blickfeld der Gedächtnisinstitutionen.
Mit Blick auf die Langzeitarchivierung sollten die Betreibenden von Repositorien die Vor- und Nachteile des Einsatzes beider Formate (TIFF-LZW und JPEG2000) sorgfältig abwägen.
Kompressionsformate sind grundsätzlich anfälliger für Bildverluste und über ihren Einsatz sollte nach einer Nutzen-Risiko-Abwägung entschieden werden.
Bei der Auswahl eines Formates ist auch seine Verbreitung und Marktdurchdringung zu berücksichtigen.
Unabhängig davon, dass international große und einflussreiche Bibliotheken wie die Library of Congress und die British Library auf JPEG2000 setzen, ist seine Verbreitung bisher geringer als die des unkomprimierten TIFF-Formates.
Die Lizenzierungssituation ist noch nicht abschließend geklärt, einige Bereiche von JPEG2000 wurden jedoch als frei deklariert. 

Auch die unterschiedlichen proprietären RAW-Formate sind nicht zur Sicherung der Bildaufnahme als Master geeignet, zumal sie oftmals nur mit der dazugehörigen RAW-Software angezeigt werden können.
Ebenso hat sich das plattformübergreifende, kameraunabhängige Adobe-RAW-Format DNG nicht durchgesetzt und ist somit als Archivformat ungeeignet. 

Für die Publikation im Internet empfehlen sich wegen ihrer großen Verbreitung grundsätzlich JPEG und PNG.
Jedoch können auch andere Formate oder Techniken zum Einsatz gelangen, wenn diese für die Präsentation z. B. von sehr großen Bilddateien praktikabler sind.
Hierbei ist darauf zu achten, dass nur Formate und Techniken eingesetzt werden, die ohne spezielle Technik oder Software von der Wissenschaft genutzt werden können. 

Für AV-Medien muss ebenso zwischen Archivformaten und Nutzungsformaten unterschieden werden.
Im Bereich Audio hat sich das Waveform Audio File-Format (WAV) in Verbindung mit der Pulse Code Modulation (PCM) [29] als Archivierungsformat etabliert.
Als Nutzungsformat hat sich mp3 (MPEG-2 Audiolayer III) durchgesetzt. 
Für Videoformate kann zurzeit noch keine eindeutige Empfehlung ausgesprochen werden.
Jedoch sollte unbedingt darauf geachtet werden, nur lizenzfreie und offen beschriebene Formate zu verwenden.
Gerade bei Videodateien liegt das Problem in der enormen Größe, so ist die Anforderung einer unkomprimierten Speicherung bei größeren Projekten kaum umsetzbar. [30] 
 
Digitale 3D-Dokumentation von digitalisierten bzw. digital rekonstruierten 3D-Objekten werden als 3D-Modelle abgelegt.
Diese Modelle ermöglichen es, die Form, die Textur und die optischen Materialeigenschaften des Ursprungsobjekts originalgetreu abzubilden, bzw. nicht mehr vorhandene (u. a. zerstörte) Objekte zu dokumentieren und zu visualisieren.

Infrage kommende Datenformate sollten möglichst robust gegenüber Beschädigungen auf den Datenträgern sein, Speicherplatz möglichst effizient nutzen, das Modell logisch strukturiert nachbilden, eine schnelle Datenverarbeitung ermöglichen und entsprechend verbreitet sein.
Als bewährtes Dateiformat im Bereich der 3D-Modelle hat sich OBJ (.obj) [31], als offenes Dateiformat, bewährt.
Das Format wird von vielen 3D-Grafikprogrammen unterstützt und ist daher geeignet für die programm- und plattformübergreifende Weitergabe von 3D-Modellen.
Optische Materialeigenschaften (z. B. Spiegelung, Transparenz, Glanzlicht usw.) werden in einer separaten Materialdatei (.mtl) definiert, die auch Angaben zu Texturierungen enthalten kann.Erwähnenswert ist ebenfalls das Format Collada DAE (.dae) [32], ein auf XML basierendes offenes Austauschformat von 3D-Datensätzen.
Dabei geht es nicht nur um die Weitergabe von Modellen und Texturen, auch Einstellungen und angewandte Veränderungsschritte sollen so von einem Programm zum anderen übermittelt werden können.
Für die web-basierte Dokumentation und Vermittlung bietet sich das speziell zur Visualisierung von 3D-Modellen innerhalb der WebGL-Technologie entwickelte Format X3D (.x3d) [33] an. 
 
[26]http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&id=686; 
http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&id=1099 
[27] http://www.jpeg.org/jpeg2000/ 
[28] ISO/IEC 15444-6:2013 
[29] Mindestqualität: 44,1 khz Abtastrate und 16 bit Abtasttiefe 
[30] Folgende Formate werden für die Langzeitarchivierung empfohlen, wobei zurzeit noch keine befriedigende Lösung vorliegt: MJPEG2000/MPEG-4 (ISO/IEC 14496-12:2015, ISO/IEC 15444-12:2015), DPX (Standardisiert als SMPTE 268M-2003, v 2.0), MXF/AAF(Standardisiert als SMPTE 377M und als ISO-Standard vorgeschlagen). 
Die FADGI arbeitet zurzeit an einer Richtlinie zur Archivierung von Filmen: http://www.digitizationguidelines.gov/guidelines/MXF_app_spec.html 
[31] http://www.fileformat.info/format/wavefrontobj/egff.htm und https://de.wikipedia.org/wiki/Wavefront_OBJ  
[32] https://www.khronos.org/collada/ und https://de.wikipedia.org/wiki/Collada_(Speicherformat)  
[33] http://www.web3d.org/x3d/what-x3d und https://de.wikipedia.org/wiki/X3D 

 
#### Materialspezifische Parameter 

##### Textwerke 

Zu den Textwerken zählen in diesem Zusammenhang sowohl gedruckte Werke als auch unikale Dokumente wie Handschriften und Archivgut. 

Bei der Altbestandsdigitalisierung ist auf jeden Fall die Imagedigitalisierung vorzunehmen. 
Auch bei Vorliegen eines maschinenlesbaren Volltexts sollte auf die Imagedigitalisierung bzw. auf die Präsentation des digitalen Faksimiles nicht verzichtet werden, da eine Fülle von Informationen nur über das visuelle Abbild transportiert werden. 

Die Lesbarkeit der Texte ist ausschlaggebend für die Wahl der Auflösung und von der im Text verwendeten Schriftgröße abhängig.
Die benötigte Scanauflösung richtet sich in diesem Fall weniger nach den Maßen der Vorlage als nach der Lesbarkeit der einzelnen Lettern.
Für Textdokumente mit dem kleinsten signifikanten Zeichen ab 1,0 mm ist eine Auflösung von 400 dpi und erst ab 1,5 mm Mindestzeichengröße eine Auflösung von 300 dpi zu empfehlen. [34] 

Die Blätter/Seiten der Textwerke werden immer vollständig mit leichtem umlaufendem Rand digitalisiert, um deutlich zu machen, dass nichts von der Vorlage abgeschnitten wurde. 


[34] Vgl. Federal Agencies Digitization Guidelines Initiative (FADGI): Technical Guidelines for Digitizing Cultural Heritage Materials: Creation of Raster Image Master Files, August 2010: http://www.digitizationguidelines.gov/guidelines/FADGI_Still_Image-Tech_Guidelines_2010-08-24.pdf, S. 59. 


##### Grafische Darstellungen 


Wie unter 3.2.1.1 beschrieben liegt die empfohlene Mindestauflösung für die Digitalisierung bei 300 dpi unter Berücksichtigung der Ergebnisse einer Testdigitalisierung mit genormter Testtafel. 

Häufig wird jedoch bei kleinen Vorlagen eine Auflösung von 300 dpi bezogen auf das Vorlagenformat nicht ausreichen, um die Eigenschaften der Vorlage umfassend erkennbar wiederzugeben.
Ist z. B. die künstlerische Technik der Ausführung nicht identifizierbar, sollte unbedingt eine höhere Auflösung gewählt werden.
Beispielhaft seien hier erwähnt: 
Kupferstiche, Briefmarken, Medaillonportraits, Miniaturmalerei. 

Die erreichbare Maximalauflösung ist dabei von der eingesetzten Digitalisierungstechnik abhängig.
Hierzu ein Rechenbeispieleispiel ohne Berücksichtigung der Objektivqualität: Eine Briefmarke ließe sich mit einer Digitalkamera mit einem Sensor im Kleinbildformat 
(24 x 36 mm) im Maßstab 1:1 reproduzieren.
Bei einem Pixelmaß der Kamera von 3800 Pixeln/cm ergibt sich hochgerechnet eine Auflösung von 4021 dpi. 

Umgekehrt kann bei überformatigen Vorlagen (DIN A0 und größer) die Auflösung bezogen auf das Vorlagenformat reduziert werden, wenn die Objekte für einen großen Betrachtungsabstand entworfen wurden.
Plakate sind hier als Beispiel zu nennen, sie sind oftmals auf Signalwirkung und Fernsicht ausgelegt.
In diesem Fall kann die Auflösung bis auf 150 dpi reduziert werden, da mit wachsendem Betrachtungsabstand auch die Punktgröße wächst, die vom menschlichen Auge nicht mehr unterschieden werden kann.
Sind Überformate jedoch detailreich wie beispielsweise Generalkarten oder Kupferstiche sollte mit mindestens 300 dpi digitalisiert werden. 

Aufgrund des Mehrwerts für die Benutzung sollte in einem ökonomisch vertretbaren Rahmen die volle Auflösung der eingesetzten Aufnahmetechnik bei den grafischen Darstellungen genutzt werden. 

Die Grafiken werden immer vollständig mit leichtem umlaufendem Rand gesichert, um deutlich zu machen, dass nichts von der Vorlage abgeschnitten wurde. 


##### Fotografien 

Bei Fotografien ist zunächst zwischen Durchsichtmedien, also fotografischen Negativen oder Diapositiven, und Aufsichtmedien, also Fotopositiven z. B. auf Papier, zu unterscheiden.
In der Regel handelt es sich bei Letzteren um Kontaktkopien oder Vergrößerungen von Negativen auf Fotopapier. 


###### Durchsichtmedien 

Das fotografische Negativ oder Diapositiv ist das Ergebnis einer fotografischen Aufnahme und damit die originale Quelle.
Negative sind die Vorlagen für die Erstellung von Reproduktionen zur Verbreitung der Aufnahme als Fotopositiv oder Druck. 

Bei Diapositiven lässt sich die Qualität eines Scans visuell mit Hilfe eine Normlichtleuchtkastens mit dem Original vergleichen.
Zur Farb- und Tonwertkorrektur sollte die Bilddatei idealerweise in 48 Bit/RGB, mindestens jedoch in 24Bit/RGB vorliegen.
Sind keine weiteren Änderungen an der Datei vorgesehen, wird das Endergebnis mit 24 Bit/RGB gesichert.
Soll vorrangig der Zustand des Vorlagenmediums zum Zeitpunkt des Scans als digitaler Master dokumentiert werden, empfiehlt sich alternativ auch eine Sicherung mit 48 Bit/RGB, besonders wenn die originale Vorlage Alterungserscheinungen aufweist, die für die diversen Derivate bearbeitungsintensiv korrigiert werden sollen. 

Anders verhält es sich mit Negativen: Die negative Darstellung ist für die Betrachtung des Motivs ungeeignet.
Um das Bildmotiv zu nutzen, muss ein positives Derivat aus dem digitalen Master erzeugt werden.
Dieses dient als digitaler Fotoabzug vom Negativ und gibt den Bildinhalt wieder.
Die positive Darstellung eines Masters ist aber im Regelfall zu flau und bei Farbnegativen zusätzlich durch deren Farbmaske nicht farbrichtig.
Um das Motiv im digitalen Fotoabzug klar erkennbar wiederzugeben, ist daher eine beträchtliche Bildkorrektur nötig.
Soll vorrangig der Zustand des Mediums zum Zeitpunkt des Scans als digitaler Master gespeichert werden, empfiehlt sich alternativ auch eine Sicherung mit 48Bit/RGB bzw. 16 Bit/Graustufen. 
Bei SW-Durchsichtmedien ist die Speicherung in Graustufen ausreichend.
Das Speichern von Farbinformationen bei SW-Vorlagen ist nur dann zu vertreten, wenn wichtige Informationen zu dem Medium in Farbe vorliegen, wie Retuschen im Negativ. 

Gut ausgearbeitete Negative geben Bilder mit einem Dichteumfang (= Tonwertumfang) von bis zu 12 Blendenstufen wieder, die so auch von der Digitalisierungsvorrichtung reproduziert werden müssen. 

Anders als die grafischen Darstellungen wird das fotografische Abbild technisch erzeugt.
Die Qualität einer fotografischen Aufnahme ist somit vom Zusammenspiel unterschiedlicher technischer Faktoren abhängig: Aufnahmeformat, Objektiv, Filmtyp, Körnigkeit der Emulsion, Entwicklung, Belichtung und Fokussierung etc. 

Um diesen Aspekten in die Bestimmung einer sinnvollen Scanauflösung einfließen zu lassen, bietet sich folgende Alternative zu dem in 3.2.1.1 beschriebenen Verfahren an: 

Außer dem Aufnahmeformat sind die Aufnahmebedingungen in der Regel nicht bekannt oder dokumentiert.
Das verwendete Filmmaterial lässt sich möglicherweise an Hand einer Kennung, wie Kerben im Planfilm, identifizieren, bei Glasnegativen entfällt dieses Merkmal jedoch. 

Um den individuellen Qualitäten eines durchmischten Bestands umfassend gerecht zu werden, kann eine Auflösung von 80 Linien/mm als Ausgangspunkt zur Festlegung der Scangrößen gewählt werden.
Das entspricht der Auflösung eines modernen feinkörnigen Diafilms.
Dadurch wird gewährleistet, dass alle Details sowohl historischer Aufnahmen als auch Aufnahmen neueren Datums im Digitalisat wiedergegeben werden.
Die Auflösungswerte moderner Filme sind größtenteils in den Datenblättern der Hersteller dokumentiert.

Eine Scanauflösung von 4000 dpi bildet die Details also ausreichend ab. [35] 
Auf das Filmmaterial bezogen wäre die theoretisch erzielbare Auflösung für alle Aufnahmeformate konstant. 

Für Kleinbildaufnahmen kann eine Scanauflösung von 4000 dpi gewählt werden, da in der Regel die Abbildungsleistung von Kleinbildoptiken besser ist als die von Mittelformat- oder Großbildobjektiven.
Für größere Aufnahmeformate kann die Scanauflösung daher verringert werden.
Einen entsprechenden Hinweis liefert der maximale Zerstreuungskreisdurchmesser eines Bildpunkts, der aufnahmeformatabhängig vom menschlichen Auge noch als scharf wahrgenommen wird.
Fotografische Praxisregel ist hierbei, dass die Formatdiagonale in mm (entspricht Normalbrennweite) mit 1/1500 multipliziert wird.
So ergeben sich folgende maximal zulässige Zerstreuungskreise je Aufnahmeformat: 

Kleinbild: 0,03 mm; Mittelformat: 0,05 mm; 9 x 12 cm: 0,1 mm; 18 x 24 cm: 0,2 mm 

Daraus abgeleitet ergeben sich folgende Zielgrößen: [36]
 
Mittelformat: 4.000 dpi x 0,03/0,05 = 2.400 dpi 

9 x 12:  4.000 dpi x 0,03/0,1 = 1.200 dpi 

18 x 24: 4.000 dpi x 0,03/0,2 = 600 dpi 

Die Fotografien werden immer vollständig mit leichtem umlaufendem Rand gesichert, um deutlich zu machen, dass nichts von der Vorlage abgeschnitten wurde. 


[35] Um die Filmauflösung in Linien/mm digital darstellen zu können, sind pro Linie mindestens zwei Pixel notwendig. 
Damit entsprechen 80 Linien/mm 160 Pixel/mm oder 1.600 Pixel/cm. 
Dieser Wert multipliziert mit 2,54 (1 inch = 2,54 cm) ergibt eine Scanauflösung von 4.064 dpi, gerundet 4.000 dpi. 
[36] Diese Werte gibt auch die FADGI vor. Vgl. Federal Agencies Digitization Guidelines Initiative (FADGI): Technical Guidelines for Digitizing Cultural Heritage Materials: Creation of Raster Image Master Files, August 2010: http://www.digitizationguidelines.gov/guidelines/FADGI_Still_Image-Tech_Guidelines_2010-08-24.pdf, S. 60. 


###### Aufsichtmedien 

Das Fotopositiv ist ein Endprodukt.
Es dient anders als das fotografische Negativ nicht als Reproduktionsvorlage oder Ausgangspunkt für eine Vergrößerung.
Die Bildqualität ist von der Negativqualität abhängig.
Anders als bei Negativfilmen gibt es zum Auflösungsvermögen von Fotopapieren (gemessen in Linienpaaren pro mm, L/mm) keine verlässlichen Angaben.
Das Auflösungsvermögen der fotografischen Emulsion im Positiv ist jedoch ausreichend für eine scharfe Bildwiedergabe bei Betrachtung mit bloßem Auge.
Als Untergrenze sollte daher eine Scanauflösung für die Digitalisate gewählt werden, die bei Ausgabe für den Druck oder auf Fotopapier die Bildqualität der Vorlage 1:1 wiedergeben kann. 

Wie unter 3.2.1.1 beschrieben, liegt die empfohlene Mindestauflösung für die Digitalisierung bei 300 dpi unter Berücksichtigung der Ergebnisse einer Testdigitalisierung mit genormter Testtafel. 

Häufig wird jedoch bei kleinen Vorlagen diese Auflösung nicht ausreichen, um die Eigenschaften der Vorlage umfassend erkennbar wiederzugeben.
In der ersten Hälfte des 20. Jahrhunderts war es gebräuchliche Praxis auch von 6 x 9 cm Negativen nur Kontaktkopien zu erstellen. [37]
 
Umgekehrt kann bei überformatigen Vorlagen (DIN A0 und größer) die Auflösung bezogen auf das Vorlagenformat reduziert werden, wenn die Objekte für einen großen Betrachtungsabstand entworfen wurden.
Großvergrößerungen sind hier als Beispiel zu nennen, sie sind auf Fernsicht ausgelegt.
Bei zu nahem Betrachtungsabstand nimmt man das Filmkorn deutlich wahr.
Hier kann die Auflösung bis auf 150 dpi reduziert werden, da mit wachsendem Betrachtungstand auch die Punktgröße wächst, die vom menschlichen Auge nicht mehr unterschieden werden kann. 

Soll allerdings bei Mappenwerken mit historischen Fotopositiven die Konfektionierung inklusive Präsentationskarton dokumentiert werden, ist von einer verringerten Auflösung abzusehen, damit Bildunterschriften und Aufnahmedetails identifizierbar bleiben, da bei großformatigen Mappenwerken der Präsentationskarton durchaus 50 % oder mehr der Gesamtfläche des Mediums einnehmen kann. 

Das Speichern von Farbinformationen bei SW-Vorlagen ist nur dann zu vertreten, wenn wichtige Informationen zu dem Medium in Farbe vorliegen, wie z. B. getonte Abzüge. 

Aufgrund des großen Mehrwerts für die Benutzung sollte in einem ökonomisch vertretbaren Rahmen die volle Auflösung der eingesetzten Aufnahmetechnik bei den Fotografien – Durchsicht und Aufsicht – genutzt werden. 

Die Fotografien werden immer vollständig mit leichtem umlaufendem Rand gesichert, um zu zeigen, dass nichts von der Vorlage abgeschnitten wurde. 


[37] So empfiehlt die FADGI für Fotoaufsichtmedien im Format 4 x 5 inch eine Auflösung von 800 dpi.Vgl. Federal Agencies Digitization Guidelines Initiative (FADGI): Technical Guidelines for Digitizing Cultural Heritage Materials: Creation of Raster Image Master Files, August 2010: http://www.digitizationguidelines.gov/guidelines/FADGI_Still_Image-Tech_Guidelines_2010-08-24.pdf, S. 62. 


##### Mikroformen 

Für Mikroformen (Mikrofilme 16 und 35 mm, Mikrofiche 105x148 mm, positiv und negativ, S/W und Farbe) gelten bezüglich der Parameter die Hinweise, die im vorletzten Abschnitt für Durchsichtmedien gegeben wurden.
Hierbei ist aber für die Auflösung abweichend zu berücksichtigen, dass es sich um Sicherungs- oder Schutzreprografien von Originalen handelt.
Die Verfilmung kann mit einem Verkleinerungsfaktor zwischen 1:7,5 und 1:96 stattfinden.
Die Auflösung bei der Digitalisierung muss sich also soweit möglich am Original orientieren und nicht am Film. 

Die Digitalisierung von Mikrofilmen wird häufig als Massendigitalisierung, die mit niedrigen Kosten realisierbar ist, eingesetzt.
Dabei lassen sich die angestrebten 300 dpi in Bezug zur Vorlagengröße nur in den seltensten Fällen realisieren, auch wenn die Auflösung des Films dafür theoretisch ausreicht.
Der limitierende Faktor sind hierbei die zurzeit marktüblichen Scanner, die in der Lage sind, teilautomatisch in kurzer Zeit ganze Filme zu digitalisieren.
Für S/W-Mikrofilme und Mikrofiche können Auflösungen bis 600 dpi bezogen auf den Film erreicht werden.
Die Wahl der Auflösung muss sich also am technisch Machbaren der Massenverfahren orientieren.
Nur in Ausnahmefällen sollte auf Reproduktionsverfahren zurückgegriffen werden, die Einzelscans mit höherer Auflösung ermöglichen. 

Liegen filmische Mikroformen vor, die nach den Standards der Bundessicherungsverfilmung erstellt wurden oder diesen entsprechen (vgl. Kap. 2.3), und dienen diese als Grundlage für eine Digitalisierung, ist zu prüfen, ob die Erstellung und Erhaltung eines Masters sinnvoll ist. 
Die Mikroverfilmung selbst ist in diesem Fall Langzeitsicherungsmedium und Kopiermaster. 
Jedoch muss immer geprüft werden, ob die Erstellung von Digitalisaten (Master und Nutzungsversionen) vom Original sinnvoller ist, als die Erzeugung von Benutzungsdigitalisaten vom Mikrofilm. 


##### Dreidimensionale Objekte 

Bedingt durch den informationstechnologischen Fortschritt der letzten Jahre gewinnt die digitale Erfassung und Vermittlung von 3D-Objekten im Kulturerbebereich laufend an Bedeutung. 
Die digitale 3D-Dokumentation erfasst dabei in erster Linie museale Sammlungsbestände sowie die Architektur und ihre Ausstattung. 

Erfolgte diese vor kurzem noch hauptsächlich durch fotografische Abbildung aus verschiedenen Aufnahmestandpunkten, so kann heute auf eine bewährte Auswahl an 3D-Erfassungs- und Rekonstruktionsmethoden sowie diesen Prozessen beistehende Technologien zurückgegriffen werden. 

Im Gegensatz zur Fotografie wird bei der 3D-Dokumentation die gesamte Geometrie eines Objekts, seine Oberflächentextur und nach Möglichkeit seine optischen Materialeigenschaften erfasst, integriert und im digitalen 3D-Modell zusammenzugeführt. 

Ein entscheidender Vorteil gegenüber einer Fotografie ist, dass hierbei die Form und die Oberflächen-Licht-Interaktion des Objekts originalgetreu erfasst und wiedergegeben werden können.
Das 3D-Modell kann aus beliebigen Perspektiven in der originalen Lichtsituation sowie in beliebigen neuen Lichtsituationen und Umgebungen visualisiert und simuliert werden.
Eine open source Anwendung zur interaktiven Web-Präsentation von hochaufgelösten 3D-Modellen bietet u. a. “3D Heritage Online Presenter” [38] (3DHOP). 

Bei der digitalen 3D-Dokumentation kann zwischen einer Retrodigitalisierung infolge der 
Überführung eines physisch vorhandenen Objekts in ein Digitalisat und einer Rekonstruktion eines physisch nicht (mehr) vorhandenen Objekts in Form eines nativ digitalen 3D-Datensatzes unterschieden werden. 

Das Ziel der Retrodigitalisierung und der digitalen Rekonstruktion dient vorrangig der Erschließung, Dokumentation, Sicherung und Archivierung sowie der web-basierten Zurverfügungstellung von Kulturerbe für die Forschung und die Öffentlichkeit.



Die digitalen 3D-Modelle zeichnen sich durch folgende Vorteile aus: 

* praktisch beliebige und parallele Verfügbarkeit (auch in unterschiedlicher Qualität) digitaler 3D-Modelle von Kulturobjekten für Wissenschaftlerinnen und Wissenschaftler 
* Ergänzung fehlender (u. a. zerstörter) Bereiche infolge der Rekonstruktion 
* Simulation verschiedener hypothetischer Versionen und/oder Varianten eines 3D-Objekts 
* Einsatz von digitalen 3D-Modellen im Museumsbetrieb, z. B. zur Ausstellungsplanung, 
Dokumentation, Beschaffungsplanung, etc. 
* virtuelle Präsentation und Ausstellung (in Kombination mit neuen Präsentationstechniken, wie z. B. hybriden Exponaten) als Mittel zum Wissenstransfer und Attraktivitätssteigerung 
* Referenz für die Restauration beschädigter Originale bzw. generierungphysischer Repliken auf Basis des digitalen 3D-Modells 
* Ersatz für Ausleihe (Vermeidung von Beschädigung, Versicherungskosten, Rechtsunsicherheit zu Eigentum) 

Auf der anderen Seite stellen uns digitale 3D-Modelle vor neue Herausforderungen.
Bislang haben sich keine Standards im Bereich der softwareunabhängigen Datenformate und der Dokumentation der Digitalisierung/Modellierung und der Resultate durchgesetzt, so dass in diesem Kontext von fehlender Interoperabilität die Rede sein kann.
Darüber hinaus verzeichnen wir einen Mangel an adäquater digitaler Forschungs- und Informationsinfrastruktur, die eine Nachhaltigkeit der 3D-Datensätze infrage stellt.
Daraus ergibt sich ein großer Handlungsbedarf, der zur Zeit u. a. durch den im November 2014 konstituierten “Rat für Informationsinfrastrukturen” [39] (RfII) definiert wird. 

Für die Retrodigitalisierung von 3D-Objekten stehen verschiedene Technologien zur Auswahl, die je nach Objektart und Eigenschaften (Größe, Material, Zustand, etc.) zu wählen sind. 
Gegenwärtig wird in dem Projekt “Colour and Space in Cultural Heritage” [40] (COSCH) versucht einen Online-Ratgeber für die Wahl einer adäquaten Methodik und Technologie je nach Problemstellung zu entwickeln.
Zielführend ist jedoch weiterhin der frühe Kontakt mit führenden Technologieentwicklern und Anwendern im Bereich der Massendigitalisierung, wie dem “CultLab3D” [41]. 

Es gibt verschiedene optische Digitalisierungsverfahren. 

Laserscanverfahren werden aufgeteilt in Lauflängenmessung und Triangulierung.
Im ersten Fall wird ermittelt, wie lange ein Laserpuls zum Objekt und wieder zurück benötigt, um die Entfernung zu diesem Punkt zu messen.
Im zweiten Fall wird mittels einer Kamera aus der Verzerrung der projizierten Laserlinie auf der Oberfläche eines Objekts auf ihren Verlauf in 3D geschlossen. 

Streifenlichtverfahren setzen auf die Projektion definierter Muster (meist parallele Linien) auf Objektoberflächen und schließen ebenfalls aus ihrer Verzerrung auf die zugrundeliegende 3D Oberflächengeometrie.
Zur Erhöhung der Auflösung können diese Muster über die Oberflächen phasenverschoben werden. 

Bei photogrammetrischen Verfahren werden aus verschiedenen Perspektiven Bilder des zu digitalisierenden Objekts angefertigt und nach Merkmalspaaren auf mindestens zwei sich überlagernden Aufnahmen durchsucht, so dass sich für diese mittels Triangulierung die Tiefe des zugehörigen Bildpunktes ergibt.
Charakteristische Merkmale können dabei beispielsweise Intensitäts- oder Kontrastunterschiede sein. 

Das Prinzip bei Time-of-Flight Kameras entspricht der Lauflängenmessung bei Laserscannern, mit dem Unterschied, dass eine ganze Szene auf einmal beleuchtet, aufgenommen und für jedes Pixel des Kamerasensors eine Tiefe berechnet wird. 

Ab bestimmten Objektgrößen, wie etwa größere Statuen oder gar Gebäude, ist eine punktweise Vermessung wie etwa mit Laserverfahren oder Mittels Oberflächenkodierung nicht mehr effizient und photogrammetrische Verfahren zeigen ihre Stärken. 

Bei Retrodigitalisierung existiert ein generelles Spannungsverhältnis zwischen Objektgröße und Auflösung, bzw. genauer zwischen dem Messbereich eines Sensors und der Genauigkeit. 
Je größer das Objekt bzw. je größer die Entfernung, desto gröber werden Details erfasst. 

Die Geometrie und Textur werden üblicherweise im selben Arbeitsschritt der Geometrieakquise erfasst, weil eine direkte Korrespondenz zwischen Messpunkten und Punkten des Bildgebers vorliegt und ein Großteil der Akquise-Methoden einen Kamerasensor als integralen Bestandteil der Erfassung verwenden, um Tiefeninformationen zu extrahieren. Dieser hat pro erfasstem Ausschnitt des Objekts meist bereits die ideale Ausrichtung auf das Objekt, so dass dieselbe Pose des Kamerasensors für die Texturgewinnung genutzt werden kann. 

Für die Erfassung der Textur ist eine ausreichende, diffuse und gleichmäßige Beleuchtung erforderlich.
Wichtig ist auch ein passender Weißabgleich, da die meist künstliche diffuse Beleuchtung der Objektoberfläche kompensiert werden muss.
Eine Farb-Kalibrierung ist hierbei wichtig, da Textur einen wesentlichen Einfluss auf die Realitätsnähe hat. 

Zusätzlich zu Geometrie und Textur lassen sich auch optische Materialeigenschaften erfassen.
Darunter versteht man beispielsweise Glanz- und Reflexionseigenschaften.
Typischerweise geschieht dies in dem man jede Kombination aus Lichteinfall und Perspektive auf ein Objekt aufnimmt, um im Anschluss die für jeden Oberflächenpunkt (Sichtbarkeitsprüfung) relevante Texturen abzuspeichern, so dass bei späterer interaktiver Betrachtung des virtuellen Objektes unter jeglicher Beleuchtung die physikalisch korrekten Texturen appliziert werden und damit die originale Materialanmutung und Licht-Oberflächeninteraktion erreicht wird.
In manchen Fällen ist eine komplette Materialakquise eines Objekts nicht möglich, entweder bedingt durch seine Größe, oder weil nur Materialreste erhalten sind.
In diesen Fällen bietet sich eine punktuelle Erfassung von Materialeigenschaften und eine anschließende Extrapolation auf Regionen gleichen oder ähnlichen Materials an. 

Die hypothetische Rekonstruktion nicht vorhandener (u. a. zerstörter) 3D-Objekte basiert auf der Erfassung und Interpretation vom historischen Quellenmaterial.
Die computergestützte Rekonstruktion bedient sich der gängigen 3D-Simulations- und Animationssoftware 
(u. a. Autodesk 3ds Max und Maya, Cinema 4D, SketchUp, Blender, etc.) bei der Erstellung der 3D-Modelle.
Als komplementäre Methode zur Retrodigitalisierung bietet uns die Rekonstruktion die Möglichkeit nicht mehr Vorhandenes zu dokumentieren und visualisieren.
Bezeichnend ist dabei die Problematik der Darstellung von Wissensunschärfen und Hypothesen. 
Hinzu gesellen sich dringend geforderte Dokumentationsstandards für die interpretative Modellierung und Visualisierung, die u. a. eine Zuweisung der Quellen und die Abbildung der kreativ-interpretativen Prozesse beinhalten sollen. 

“Die digitale Erschließung […] zielt auf die Integration und Konvergenz zwischen digitalisierten und nativ digitalen Daten in einheitlichen, integrierten Arbeitsumgebungen mit dem Ziel dynamischer Wissensintegration” [42], wie der Rat für Informationsinfrastruktur bei der Erläuterung zur Informationsinfrastruktur feststellt.
Einen wichtigen Beitrag zur Integration und Konvergenz leisten die strukturierten Daten, welche sich an einschlägigen Metadata-Schemata (z. B. LIDO und CARARE 2.0), kontrollierten Vokabularen (z. B. Getty Vocabularies) und Normdateien 
(z. B. GND) sowie Referenzontologien (allen voran CIDOC (Documentation International Committee for Documentation) CRM) richten.
Die Erweiterung von CIDOC CRM zur besseren Erfassung der Retrodigitalisierung fand innerhalb von CIDOC CRM dig ihren Ausdruck.
Die Vorstellung von CIDOC CRM referenzierten Applikationsontologien zur Wissensrepräsentation einer computer-gestützten Rekonstruktion werden gerade in der Fachgemeinschaft vorgestellt und diskutiert. [43]
 
[38] http://3dhop.net/index.php  
[39] http://www.rfii.de/de/index/ 
[40] http://cosch.info/ 
[41] http://cosch.info/ 
[42] RfII Berichte No. 1: Begriffsklärungen (2016), S. 13, auf http://www.rfii.de/de/category/dokumente/  
[43] http://www.digitale-rekonstruktion.info/ 


### Metadaten 

Die Erzeugung von Metadaten, welche erst die Auffindbarkeit der Objekte gewährleisten und eine kontextualisierende Präsentation ihrer digitalen Images erlauben, ist zentraler Bestandteil der Digitalisierung. 
Die DFG geht davon aus, dass die der Digitalisierung zu Grunde liegenden analogen Objekte bereits primär in anerkannten digitalen Nachweissystemen erschlossen sind bzw. mit der Digitalisierung einhergehend erschlossen werden. 
Metadaten, die im Rahmen des Digitalisierungsprojekts entstehen, sind grundsätzlich in einer von der Software unabhängigen und standardkonformen Form bereitzustellen, in aller Regel in einer XML-Kodierung. 
Dies ist in den Workflow des Projekts so einzubetten, dass ein vollständiger Metadatensatz in software-unabhängiger Form auch dann bereitsteht, wenn das Projekt – aus welchen Gründen auch immer – zu einem vorzeitigen Ende kommt. 

Wenn sich die im Rahmen eines geförderten Digitalisierungsprojekts ausgewählten Objekte sachlich für die Einbindung in ein material- oder fachspezifisches Portal eignen, wird erwartet, dass ein Projektantrag entweder erläutert, welche Vorkehrungen projektseitig getroffen werden, um die Anbindung an dieses Portal während und nach der Projektlaufzeit sicherzustellen, oder plausibel macht, warum eine Anbindung aus inhaltlichen Gründen oder aus Gründen des damit verbundenen Aufwands nicht notwendig bzw. sinnvoll ist. 

Allgemein werden deskriptive (bibliografische Beschreibung, archivische Erschließung, Beschreibung von unikalen, häufig zudem nicht-textuellen Objekten), strukturelle (Text-, Dokumentstruktur), administrative (z. B. Rechteverwaltung) und technische (z. B. Dateitypen) Metadaten unterschieden. 
Die folgenden Überlegungen beziehen sich ausschließlich auf deskriptive und strukturelle Metadaten. 

Die Verknüpfung zwischen den Metadaten einerseits und den digitalen Images andererseits zu einem Objekt muss dabei immer auf der Ebene der Metadaten gewährleistet sein. 
Zusätzlich können Metadaten auch in den Header der digitalen Images eingebettet werden, jedoch werden diese von den Software-Produkten unterschiedlich dargestellt und im schlimmsten Fall sogar korrumpiert, so dass die Einbettung in jedem Fall nur eine ergänzende Option ist. 

Die Metadatenformate realisieren die Referenzierung der digitalen Images auf unterschiedliche Weise. 
Mit dem Containerformat METS können sowohl deskriptive Metadaten über Mappings in Standardformaten wie z. B., MODS oder TEI als auch strukturelle Metadaten inklusive der Referenzen auf die digitalen Images transportiert werden. 
Dies bietet sich insbesondere für Volldigitalisate von Textwerken an, die sowohl mit deskriptiven als auch strukturellen Metadaten erschlossen sind. 
Andere Formate wie LIDO enthalten zusätzlich zu den semantischen Elementen zur Objektbeschreibung eigene Elemente zur Referenzierung beliebiger digitaler Ressourcen zum Objekt (neben Images auch Audio- oder Videodaten). 
Auch hier können beliebig viele Ressourcen mit einem Objekt verknüpft und noch mit eigenen Beschreibungselementen versehen sein.
Auch in EAD(DDB) ist eine Verknüpfung zu beliebig vielen digitalen Ressourcen einschließlich der Einbindung von METS-Containern möglich.
Wichtig ist, dass die digitalen Ressourcen innerhalb der Metadaten über global eindeutige, persistente Adressen, in der Regel URLs, referenziert werden (vgl. persistente Adressierung in Kapitel 5). 

Der Nachweis der Digitalisate und Metadaten muss bei Bibliotheksgut entweder durch Katalogisierung der elektronischen Ausgabe oder durch Angabe der PURL der Bilddateien bzw. Angabe einer persistenten Verknüpfung im Katalog (OPAC, Verbundsystem) erreicht werden.
Von Antragstellerinnen und Antragstellern aus universitären Einrichtungen wird erwartet, dass sie die Frage der Katalogisierung mit ihren örtlichen Bibliotheken abstimmen bzw. durch diese durchführen lassen. 
Der Nachweis digitalisierter Drucke im ZVDD wird erwartet.
Die Digitalisate anderer bibliothekarischer Objekte sind in einschlägigen materialspezifischen Portalen nachzuweisen (z. B. Manuscripta Mediaevalia für mittelalterliche Handschriften [44], Kalliope für Nachlässe [45]). 
Materialien, die nicht in bibliothekarische Nachweisinstrumente eingebracht werden können, sollten in geeigneten fachlichen oder fachübergreifenden Online-Anwendungen präsentiert werden.
Der Nachweis der Digitalisate und Metadaten muss bei Archivgut im Archivportal-D bzw. in der DDB erfolgen. 
Die Einbringung der Daten in die DDB und über die DDB in die Europeana wird von allen Projekten erwartet.

Es obliegt dem Projektnehmer sicherzustellen, dass die im Projekt hergestellten Digitalisierungseinheiten eindeutig identifiziert und von anderen im selben System vorhandenen Einheiten getrennt durchsucht und aufgerufen werden können. 

Die Ablieferung an diese Portale sollte gemäß den Standardformaten wenn möglich über OAI erfolgen (vgl. Kapitel 3.2.1.4). 


[44] http://www.manuscripta-mediaevalia.de/ 
[45] http://kalliope.staatsbibliothek-berlin.de/de/index.html. 


#### Erschließung, deskriptive Metadaten 

Die Erschließung eines zu digitalisierenden Objekts mit deskriptiven Metadaten erfüllt je nach Material unterschiedliche Funktionen für die Bereitstellung.
Zunächst ermöglichen es die deskriptiven Metadaten überhaupt erst, die Inhalte gezielt durch eine Recherche aufzufinden. 
Das Objekt wird klassifiziert, historisch kontextualisiert und gegebenenfalls in seiner Materialität beschrieben.
Im Idealfall bieten die deskriptiven Metadaten Anknüpfungspunkte für unterschiedliche Fragestellungen und Fachdisziplinen.
Digitalisierung ohne Nachweis von Metadaten nach den gängigen Community-Standards ist nicht förderfähig. 

Sofern in einem Digitalisierungsprojekt über bereits vorliegende deskriptive Metadaten hinaus eine projektspezifische Erschließung und/oder Informationsbereitstellung geplant ist – und dies wird zumindest im Bereich unikaler Objekte der Regelfall sein – sind die wichtigsten Fragestellungen an das Material zu antizipieren und in einem Kernfeldkatalog zu formulieren.
Die systematische Bearbeitung der Metadaten auf der Grundlage eines Kernfeldkatalogs ist Voraussetzung für eine optimale Bereitstellung der digitalisierten Bestände gemäß Kap. 6.2. 

Für eine optimale verteilte, auch nachhaltige Nutzbarkeit der Metadaten ist die Erschließung an den einschlägigen Spartenstandards (z. B. RDA [46], ISAD (G) [47]) und Referenzmodellen (CIDOC-CRM [48], perspektivisch auch IFLA FRBR/FRBRoo [49]) zu orientieren und wo immer möglich mit publizierten Normdaten zu verknüpfen.
Ausdrücklich wird für die Erfassung personenbezogener, biografischer Information und für den geografischen Nachweis die Verwendung der Gemeinsamen Normdatei (GND) der Deutschen Nationalbibliothek empfohlen. 
Darüber hinaus eingesetzte kontrollierte Vokabulare, z. B. Iconclass [50] zur Bilderschließung, sollten national und international anschlussfähig sein. 

Die Bereitstellung der Metadaten zur weiteren Nutzung gemäß den materialspezifischen Standards ist verpflichtend: METS/MODS für gedruckte Textwerke (s. Anhang A), METS/TEI für Handschriften (s. Anhang B), EAD(DDB) [51] in Verbindung mit einem METS/MODS-Mapping für Archivmaterial (s. Anhang A), LIDO für (i.d.R. unikale) bildhafte und dreidimensionale Objekte (s. Anhang C). 
Die bereitgestellten Metadaten müssen gegen das jeweilige XML Schema valide sein und sind darüber hinaus auf semantische Korrektheit zu überprüfen. 


[46] https://wiki.dnb.de/display/RDAINFO/RDA-Info. 
[47] http://www.ica.org/sites/default/files/CBPS_2000_Guidelines_ISAD%28G%29_Second-edition_DE.pdf. 
[48] CIDOC Conceptual Reference Model: http://cidoc-crm.org/ 
[49] IFLA Functional Requirements for Bibliographic Records: http://www.ifla.org/en/publications/functional-requirements-for-bibliographic-records 
[50] http://www.iconclass.nl/ 
[51] http://www.landesarchiv-bw.de/web/53401. 


#### Strukturelle Metadaten für digitale Faksimiles 

Wohl zu erwägen ist die Frage der Anwendung von strukturellen Metadaten zur Erschließung von Images, also der Kodierung der strukturellen Elemente eines Dokumentes, wie z. B. Widmung, Vorwort, Kapitel oder Illustration.
Die Aufnahme dieser Aspekte nimmt den Gedanken der analytischen Bibliografie auf, die an den Kapitel- und Textstrukturen entlang den Inhalt eines Werkes referiert.
In manchen Fällen ist die Erstellung von strukturellen Metadaten von eher nachgeordneter Bedeutung, in anderen Fällen ist die Erstellung eines solchen künstlichen Inhaltsverzeichnisses für die Navigation im Digitalisat unverzichtbar.
Beispielsweise ist es keinem Nutzer zuzumuten, in einem 600 Seiten umfassenden digitalen Wörterbuch online zu blättern, um den richtigen Alphabeteinstieg zu finden.
Bei manchen Fragestellungen sind strukturelle Metadaten auch für die Recherche interessant.
Die Entscheidung über die Erstellung struktureller Metadaten ist daher immer eine material- und projektspezifische. 

Falls strukturelle Metadaten Verwendung finden, wird die Konsultation der über die Website des DFG-Viewers zugänglichen Liste von Bezeichnungen empfohlen.
Sollten darüber hinaus noch andere Bezeichnungen erforderlich sein, sollte man sich im Rahmen konkreter Digitalisierungsprojekte über standardisierte Bezeichnungen verständigen und diese meist fach- oder sachbezogenen Vokabularien über die Projektwebsite und gegebenenfalls auch die Webseite des DFG-Viewers publizieren, um eine umfassende Nachnutzung zu ermöglichen. 

Bei der Vergabe von strukturellen Metadaten stellt sich die Frage, ob man sich bei der Erschließung der Dokumente eher am digitalen Faksimile, der physikalischen Seitenfolge, oder an der Text- bzw. Kapitelstruktur des Werks orientiert.
Geht man davon aus, dass dem digitalen Faksimile eines alten Drucks oder einer Handschrift die Transkription oder Edition beigegeben werden soll, ist eine Orientierung am TEI-Kodierungsstandard zu empfehlen.
Bei einer Seitenbeschreibung mit einigen qualifizierenden Merkmalen (z. B. Illustrationen oder Annotationen) ist eher der von der Library of Congress gepflegte METS zu empfehlen.
Für beide Modelle, die seitenorientierte und die dokumentorientierte, gibt es gute Argumente.
Meist lassen sich beide Ansätze auch ineinander überführen.
Tendenziell ist eine Struktur, die der Logik des Textes folgt, leistungsfähiger, was spätere Abfragemöglichkeiten und Repräsentation der Vorlage anlangt.
Allerdings wird dieser Vorteil durch einen höheren technischen Aufwand bei der Bearbeitung und Präsentation der Dokumente erkauft.
Es sei darauf hingewiesen, dass auch eine auf die physikalische Seitenfolge bezogene Kodierung, wie sie in Bibliotheken eher anzutreffen ist, die Verwendung von TEI nicht ausschließt, [52] so dass sich beide Aspekte gegebenenfalls auch sinnvoll verbinden lassen. 

Empfohlen wird nach dem derzeitigen Stand bei alten Drucken eine Orientierung an METS oder TEI.
Dessen ungeachtet soll in jedem Fall der DFG-Viewer unterstützt werden, der auf METS beruht.
Wenn daher TEI für Strukturdaten zum Einsatz kommt, ist im Rahmen des Projekts eine Konversion zu METS notwendig.
Da die Basis in beiden Fällen XML ist und die beschriebenen Sachverhalte ähnlich, kann man davon ausgehen, dass diese Konversion keine größere Hürde darstellt. 


#### Sammlungs- und Bestandsbeschreibung 

Sammlungs- und Bestandsbeschreibungen kontextualisieren und verorten das Einzelobjekt und ermöglichen dem Nutzer/der Nutzerin so einen Überblick über den Gesamtbestand einer Einrichtung.
Grundlage für ein Digitalisierungsprojekt können auch virtuelle Bestände- und Sammlungen (z. B. sachthematisch, materialspezifisch) sein. 

Es wird erwartet, dass Digitalisierungsprojekte mindestens Gegenstand und Umfang der jeweiligen Material- bzw. Objektauswahl auf einer Seite im Netz, möglichst auch in Englisch, darstellen.
Sinnvoll dafür ist eine normierte Beschreibung in XML, um diese Informationen in Zukunft besser in nationalen oder internationalen Portalen zusammenführen und gezielt recherchieren zu können.
Diese Beschreibung kann gemäß dem Dublin Core Collections Application Profile (s. Anhang D) oder in demselben Metadatenstandard erfolgen, in dem auch die Objektbeschreibungen verfügbar gemacht werden: METS, MODS oder TEI-Header, EAD(DDB) sowie LIDO bieten entsprechende Möglichkeiten.
Darüber hinaus sollte für eine Sammlung die eindeutige Identifizierung und Beschreibung gemäß ISO 27730, Information and Documentation – International Standard Collection Identifier (ISCI) erwogen werden, die auf der ISIL einer Institution aufbaut. 


#### Austausch und Weitergabe der Metadaten 

Für den Aufbau übergreifender Nachweissysteme ist die Schaffung eines Standards zum Austausch von Metadaten für Digitalisate von zentraler Bedeutung (vgl. Kapitel 6).
Allerdings können sich Standards nur jeweils innerhalb der jeweiligen Community ausbilden und etablieren.
Dabei können ein und dieselben Ressourcen durchaus im Horizont ganz verschiedener Fragestellungen erscheinen und entsprechend divergierende Sets von Metadaten erfordern. 
Ein generalisiertes Verfahren zum Austausch von Metadaten muss daher flexibel unterschiedliche Metadatenformate bzw. communityabhängige Spezifikationen verwalten können.
Dieses lässt sich mit dem Protokoll der OAI gut erreichen.
Die Verwendung von OAI ist mit Blick auf alte Drucke und Handschriften vor allem als technisches Austauschprotokoll sinnvoll.
OAI schreibt vor, dass mindestens Dublin-Core-Daten geliefert werden müssen; das ist zwar für die deskriptive Beschreibung sowohl alter Drucke und Handschriften [53] als auch nichttextueller Objekte [54] ungenügend, als zusätzliche Information aber von Nutzen.
Der OAI-Standard sieht die parallele Unterstützung weiterer Metadatenformate explizit vor; eine Verbindung von OAI mit allen XML-basierten Metadatenformaten ist daher grundsätzlich möglich (METS/MODS mit spartenspezifischen Mappings, METS/TEI-HSS, MARCXML, MABxml, EAD(DDB), TEI P5, LIDO etc.). 

Die DFG setzt verpflichtend die Bereitstellung von Metadaten für Nutzer/-innen über OAI voraus.
Eine solche kann auch über ein übergreifendes Portal (z. B: ZVDD, DDB, Archivportal–D) erfolgen.
Dabei müssen neben den von OAI vorgeschriebenen Dublin-Core-Metadaten materialspezifische Metadaten nach METS/MODS für alte Drucke, METS/MODS für Archivgut mit einem für den DFG-Viewer angepassten Mapping, METS/TEI für mittelalterliche Handschriften und LIDO für bildhafte und dreidimensionale Materialen ausgeliefert werden (vgl. auch Kapitel 7 und Anhang A).
Eine Abweichung von dieser Verpflichtung muss im Antrag ausdrücklich begründet werden. 


[52] http://www.tei-c.org/Sample_Manuals/bestpractice.htm 
[53] Vgl. Hillmann, Diane I.: Choices: MARC or Dublin Core? In: Anne R. Kenny/Oya Rieger (Hrsg.): Moving Theory into Practice. Digital Imaging for Libraries and Archives. Mountain View: Research Library Group 2000, S. 89f.  
[54] Die unzulässig vereinfachende Beschreibung musealer Objekte in Dublin Core und deren Darstellung in Online-Umgebungen war der zentrale Anlass für die Museums-Community zur Entwicklung des Harvestingformats LIDO. 


### Volltextgenerierung 

Eine vielseitige wissenschaftliche Nachnutzbarkeit, die unter anderem automatisierte Recherchen, quantitative Auswertungen im Rahmen von Text- oder Datamining, semantische Analysen, Mustererkennungen in nicht textuellen Materialien, Anreicherungen, Kontextualisierungen und Weiterverarbeitungen – auch im Rahmen von virtuellen Forschungsumgebungen – ermöglicht, basiert auf der leichten und ungehinderten Nachnutzbarkeit der entsprechenden Daten, entsprechenden Rechtseinräumungen und dem Angebot des digitalen Volltextes.
Wo immer möglich und sinnvoll, wird die DFG-Förderung daher im textuellen Bereich auf die Bereitstellung maschinenlesbarer Volltexte abzielen.
Von allen Anträgen zur Digitalisierung textueller Materialen wird daher eine Auseinandersetzung mit Möglichkeiten der Volltextbereitstellung erwartet.
In diesem Zusammenhang wird auf das Koordinierungsprojekt OCR-D [55] verwiesen, das Verfahren zur OCR-gestützten Volltexterfassung prüft und prototypische Workflows entwickelt.
Ergebnisse dieses Vorhabens sollen in die nächste Version der Praxisregeln einfließen.
Unabhängig von diesen Entwicklungen gilt für Druckwerke ab Erscheinungsjahr 1850 verpflichtend, dass Volltext hergestellt werden muss und eine bloße Bilddigitalisierung nicht ausreicht. 

Die Herstellung von Volltext kann auf mehreren Wegen erfolgen: entweder durch OCR oder durch Abschreiben bzw. Transkription.
Die Frage, welches Verfahren gewählt werden soll, ist u. a. abhängig vom Alter und Zustand der Vorlage und der notwendigen Fehlerfreiheit des Textes für die angestrebten Ziele. 


#### Texterfassung 

##### Textgenauigkeiten 

Unabhängig davon, ob man einen Text durch OCR oder manuelle Transkription herstellt, ist die Frage zu beantworten, welche Qualität für welche Zwecke benötigt wird und welche Kosten für das jeweilige Ziel angemessen sind.
Je nach Projekt werden die Anforderungen an die Textgüte variieren.
Ein Editionsprojekt wird allerhöchste Maßstäbe anlegen, während ein Massendigitalisierungsprojekt, das Tausende von Titeln umfasst, auf eine vergleichsweise teure manuelle Transkription verzichten muss.
Was dabei noch als ausreichend gilt und welche Kosten für welche Qualität akzeptabel sind, muss in Projekten fallweise, materialabhängig und nach Maßgabe der wissenschaftlichen Anforderungen sorgfältig begründet werden.
Gewöhnlich wird die Qualität von mit OCR bearbeiteten Texten in Prozent angegeben.
Dabei herrscht wenig Einigkeit darüber, welche Messkriterien und -verfahren angelegt werden.
Genauigkeiten können sich auf die Richtigkeit der Buchstaben, aber auch auf die Richtigkeit der Wörter beziehen.
Im ersten Fall ist bei 99 % jeder hundertste Buchstabe falsch, im letzteren jedes hundertste Wort.
Ob falsche Layoutinformationen (Marginalie zwar richtig erkannt, aber an der falschen Stelle eingeordnet) oder fehlende Worttrennungen als Fehler gelten, wird von Fall zu Fall anders bewertet. 

Um eine gewisse Einheitlichkeit bei der Beurteilung der Genauigkeit zugrunde legen zu können, werden Antragstellende gebeten, diesbezügliche Angaben hinsichtlich der Buchstabengenauigkeit zu machen, d. h. mangelhafte Worttrennungen und Layoutfehler unberücksichtigt zu lassen.
Es ist also zu überprüfen, wie viele Zeichen der Quelle, einschließlich der Interpunktionszeichen, korrekt erkannt wurden.
Ideal sind Messungen auf der Basis zuverlässiger Referenztexte, jedoch stehen diese nicht immer zur Verfügung, so dass auf Stichproben zurückgegriffen werden muss. 

Um transkribierte oder mit OCR erstellte Texte auf ihre Genauigkeit hin zu überprüfen, wird hier unbeschadet späterer neuer Empfehlungen, die z. B. aus den Ergebnissen des OCR-D Projektes kommen, ein statistisches Verfahren empfohlen.
Ziel ist, mittels einer Stichprobe zu überprüfen, ob eine vom Dienstleister behauptete Erkennungsquote als korrekt eingestuft werden kann, wobei einerseits die Wahrscheinlichkeit für einen Irrtum möglichst gering gehalten werden soll, andererseits aber die Stichprobengröße noch praktikabel ist.
Das dazu erforderliche statistische Verfahren ist ein so genanntes Bernoulli-Experiment.
Da die Berechnung relativ kompliziert ist, wird hier mit fest vorgegebenen Werten gearbeitet, die typische Angaben enthalten.
Vorgeschlagen wird eine Stichprobegröße von 500 beliebigen Zeichen; zu empfehlen ist die Benutzung eines Zufallsgenerators, der die Position der Zeichen bestimmt 
(1. Zeichen: 15. Seite, 24. Zeile, 7. Zeichen. 2. Zeichen: 73. Seite, 3. Zeile, 32. Zeichen usw.). 
Unter dieser Voraussetzung gilt folgende Tabelle: 

| Behauptete Erkennungsquote | Mindestzahl der korrekt erkannten Zeichen (Stichprobengröße = 500) |
|---|---|
| 95 % | 485 |
| 96 % | 489 |
| 97 % | 493 |
| 98 % | 496 |
| 99 % | 499 |
| > 99 % | 500 |
 
In der linken Spalte ist die behauptete Erkennungsquote angegeben, in der rechten die Zahl der in der Stichprobe mindestens korrekt erkannten Zeichen, die vorliegen muss, um überprüfen zu können, ob eine von einem Dienstleister behaupte Erkennungsquote als korrekt eingestuft werden kann.
Wenn also ein Dienstleister behauptet, dass ein Text eine Genauigkeit von 96 % hat, müssen in der Stichprobe von 500 Zeichen mindestens 489 Zeichen korrekt erkannt werden, damit bei einer Irrtumswahrscheinlichkeit von 2,5 % die Behauptung des Dienstleisters akzeptiert werden kann.
Eine Genauigkeit unter 95 % sollte möglichst nicht vereinbart werden. 
 
Bei einer behaupteten Erkennungsquote von über 99 % müsste die Stichprobegröße erhöht werden.
Nachstehend zwei Tabellen, die beispielhaft zeigen, wie hoch die ermittelte Erkennungsquote in Abhängigkeit von einer bestimmten Stichprobengröße sein muss, wenn Texte eine behauptete Erkennungsquote von 99,5 % bzw. 99,7 % haben: 

Behauptete Genauigkeit: 99,5 % 

| Stichprobengröße | Mindestzahl der korrekt erkannten Zeichen |
|---|---|
| 500 | 500 |
| 1000 | 999 |
| 2000 | 1996 |
| 5000 | 4985 |
| 10000 | 9960 |
 
Behauptete Genauigkeit: 99,7 % 

| Stichprobengröße | Mindestzahl der korrekt erkannten Zeichen |
|---|---|
| 500 | 500 |
| 1000 | 1000 |
| 2000 | 1998 |
| 5000 | 4995 |
| 10000 | 9990 |
 
Natürlich ergeben sich hier pragmatische Grenzen im Aufwand der Stichprobenerhebung.
Die Frage, welche Genauigkeit gut, ausreichend oder mangelhaft ist, hängt von der konkreten wissenschaftlichen Fragestellung ab und lässt sich nur aus den konkreten Projektanforderungen her beantworten. 


###### OCR 

Die OCR-Technik hat in jüngerer Zeit erheblich Fortschritte gemacht, und wesentliche Verbesserungen konnten auch bei Frakturschriften sowohl der Handpressen- als auch Maschinenpressenzeit erzielt werden.
Gleichwohl können die Praxisregeln angesichts der dynamischen Entwicklung und mit Blick auf die zu erwartenden Ergebnisse des OCR-D Projektes zur Frage der OCR-Software und ihrer Anwendbarkeit derzeit noch keine abschließenden Empfehlungen geben. 

Der OCR-Prozess selbst kennt verschiedene Stufen, die aufeinander aufbauen.
Grob unterscheiden kann man das Preprocessing, in dem ein Image vorbearbeitet wird (cropping, despeckle, deskewing, binarisation), die Optical Layout Recognition (Segmentation bzw. Identifikation von Bild- und Textregionen, Strukturanalyse), Optical Character Recognition (eigentliche Texterkennung) und Postprocessing (Textkorrektur).
Insofern hat die Qualität des Ausgangsbilds ganz entscheidenden Einfluss auf den Prozess der Binarisierung und dieser wiederum auf das Erkennungsergebnis.
Daher sollten nur solche digitalen Vorlagen einer OCR-Bearbeitung unterzogen werden, bei denen sichergestellt ist, dass die Bildqualität ausreicht. 
Problematisch sind in diesem Zusammenhang auch intrinsische Phänomene wie Verschmutzungen, Widerdrucksschatten, manuelle Unterstreichungen oder Annotationen u.ä., die sich nachteilig auf den OCR-Prozess auswirken.
Der Binarisierung folgt die Segmentierung, bei der die Software versucht, die jeweiligen Textbereiche auf einem Image zu identifizieren, um die eigentlichen Textbereiche von Illustrationen oder anderen Bildelementen zu trennen.
Als besonders problematisch erweisen sich bei diesem Prozess Marginalien in älteren Drucken oder auch Zeitungen mit komplexem Layout.
Danach folgt der eigentliche OCR-Prozess der Texterkennung, der durch einen nachgeordneten Prozess der Textverbesserung (automatisch mit Hilfe von Wörterbüchern oder manuelle Korrektur) unterstützt werden kann. 

Um Daten aus der OCR leicht nachnutzen zu können, wird die Verwendung des Standards ALTO (Analyzed Layout and Text Object) empfohlen, der von der Library of Congress gepflegt wird. [56] 


###### Manuelle Erfassung/double keying 

Man unterscheidet bei der Direkterfassung – dem Abschreiben – von Texten zwei Verfahren, das einfache und das double key-Verfahren.
Bei Letzterem wird ein Text zweimal erfasst und die Abweichungen beider Versionen durch automatischen Textabgleich herausgefiltert.
Auf diese Weise sind Erfassungsgenauigkeiten von 99,97 % erreichbar, also ein praktisch fehlerfreier Text.
Bei dieser Art Erfassung sollte man sich durch hohe Prozentsätze von Anbietern nicht irreführen lassen.
Unterhalb einer Genauigkeit von 99,5 % ist bei manueller Erfassung ein Ergebnis ungenügend. 

Sollte die Erfassung durch einen Dienstleister vorgenommen werden, so muss eine brauchbare Textgenauigkeit als Zielvorgabe auch vertraglich fixiert werden.
Diese Vorgabe ist an Stichproben des digitalisierten Texts zu überprüfen (s.o.). 

Manuelles Erfassen bietet zwar eine hohe Genauigkeit, ist aber auch kostenintensiv, so dass Vor- und Nachteile gegenüber einer OCR Erfassung abgewogen werden müssen.
Die eigentliche Texterfassung wird hierbei zumeist im Ausland vorgenommen; der Kontakt mit einer Digitalisierungsfirma sollte jedoch über einen Ansprechpartner in Deutschland erfolgen, da in der Regel eine enge Kooperation und Absprache zu den Erfassungsdetails erforderlich ist. 

In einem ersten Schritt muss durch das jeweilige Digitalisierungsprojekt festgelegt werden, welche Eigenschaften der Vorlage mittels eines strukturellen Markups erfasst werden sollen. 
Hierbei können nur solche Eigenschaften ausgezeichnet werden, die grafisch unterscheidbar sind.
Einfache Strukturen können vom Dienstleister automatisch erkannt werden, bei weitergehenden Angaben müssen diese im Bild vor der Übergabe an den Dienstleister entsprechend markiert werden.

Das verursacht einen entsprechenden Personalaufwand und muss bei der Kalkulation des Projekts berücksichtigt werden. 

Da die meisten Dienstleister den Text nach Zeichenmenge inklusive des Markups berechnen, ist es ratsam, eine zeichenarme Auszeichnungsvariante [57] für diese Zwecke zu verwenden. 
 
 
[55] OCR-D: Koordinierungsprojekt zur Weiterentwicklung von Verfahren der Optical Character Recognition (http://www.ocr-d.de/) 
[56] http://www.loc.gov/standards/alto/ 
[57] Z. B. teitite: http://www.tei-c.org/release/doc/tei-p5-exemplars/html/tei_tite.doc.html 


### Zeichenkodierung 

Alle verbreiteten Betriebssysteme unterstützen Unicode.
Unicode ist auch das Zeichenkodierungsformat von XML, das die Grundlage für die wichtigsten Strukturdatenauszeichnungssysteme darstellt.
Daher wird empfohlen, die Texte in Unicode abzuspeichern.
Zu favorisieren ist das bei europäischen Sprachen sparsamere UTF-8.
Zeichen, die nicht im Unicode-Standard enthalten sind, können durch Nutzung des private plane-Bereichs von Unicode [58] abgebildet und durch entsprechende Grafiken oder Fonts repräsentiert werden.
In allen Fällen sind Möglichkeiten der Standardisierung zu prüfen. 

Die Kodierung von langem und kleinem s in Frakturschriften oder Ligaturen in Frakturschriften 
(ch, tz etc.) bzw. die Darstellung von Diphtongen (æ etc.) ist abhängig von fachspezifischen Anforderungen oder editorischen Entscheidungen, die nicht Gegenstand dieser Empfehlungen sein können, aber bei der Erstellung von Volltext im Blick zu behalten und idealerweise zu dokumentieren sind. 


[58] http://de.wikipedia.org/wiki/Unicode 


#### Markup von Volltexten 

Wenn nicht triftige Gründe dagegen sprechen, müssen Volltexte von Drucken und Handschriften nach dem Modell der TEI kodiert bzw. mit Markup versehen werden.
Als transparentes XML-Format ist TEI, sofern sorgfältig dokumentiert, auch für die Langzeitarchivierung die prospektiv beste Wahl.
Von PDF/A sollte trotz bestehender ISO-Normen (19005-1:2005 bzw. 19005-2:2011) abgesehen werden, da deren Nachnutzung gerade durch die digitale arbeitenden Geistes- und Kulturwissenschaften mangels struktureller Auszeichnung beschränkt ist.
Dessen ungeachtet ist PDF – neben zunehmend auch ePub für mobile Geräte – als derivatives Format für z. B. dynamisch generierte Lesefassungen oder für den Druck aufbereitete Texte gut geeignet und sollte wegen seiner weiten Verbreitung auch zusätzlich von digitalen Bibliotheken angeboten werden (vgl. z. B. die Angebote bei archive.org). 

Bei der Kodierung von XML-Strukturen in TEI-Dokumenten muss zunächst entschieden werden, wie und in welchem Umfang man textsortenspezifische Gliederungen wie z. B. Kapitel, 
Unterkapitel, Jahresbände, Aufsätze etc. berücksichtigt.
Hinzu kommen weitere denkbare Strukturmerkmale, z. B. Inhaltsverzeichnisse, Register, Zeilenumbruch, Spaltenumbruch, Seitenumbruch, Kopfzeile/Fußzeile/Kolumnentitel, Seitenzahl, Bilder oder bildähnliche Elemente, Bildunterschriften, Marginalien, Schriftwechsel, z. B. der Wechsel von Fraktur zu Antiqua (etwa für fremdsprachige Zitate), der Wechsel der Schriftgröße, Wechsel der Schriftart (recte, kursiv, fett usw.) u. a. m., Formeln, z. B. mathematische (MathML) oder chemische (CML) Formeln, Fortsetzungsmarkierungen (Kustoden) am Fußende von Seiten (für Anschlussbogen) etc. 

Die Wahl des Markups unterliegt in der Regel projektspezifischen Besonderheiten.
Um die Austauschbarkeit und Nachnutzung von auf diese Weise mit Markup versehenen Volltexten sicherzustellen, sollten daher die verwendeten XML-Elemente und Attribute im TEI-Header dokumentiert werden.
Besondere Beachtung verdienen in diesem Zusammenhang die Bemühung der TEI Community um ein strikt formuliertes Austauschformat: TEI–Simple. [59] 


[59] https://github.com/TEIC/TEI-Simple 


#### Layout 

In manchen Fällen ist es bei der Präsentation eines Volltexts wichtig, das Layout eines Dokumentes langfristig zu sichern.
Die Praxisregeln empfehlen für diese Fälle bevorzugt den Einsatz einer geeigneten Formatierungssprache (z. B. XSLT, CSS), die die Unabhängigkeit von spezieller Software weitgehend sicherstellt.
Falls eine Archivierung des Formats mit XML-Techniken aus nachvollziehbaren Gründen nicht möglich ist, können Layoutinformationen zu Textdokumenten auch in PDF nach der ISO-Norm 19005-1 archiviert werden.
PDF-Dateien sind aber, wie in 3.4.2. dargelegt, kein Ersatz für eine Bereitstellung des mit Markup versehenen Volltextes in XML. 

Das Wesen einer Publikation als XML + Formatierungssprache bringt es mit sich, dass dynamische Anzeigen nach Maßgabe des jeweils vom Nutzer gewünschten Zwecks generiert werden können.
Dies sollte bei der Präsentation berücksichtigt und ein möglichst breites Spektrum angeboten werden.
Typische Ausgabeformate sind z. B. HTML/XHTML, PDF, ePub oder plain text. 


### Langzeitverfügbarkeit 

Die Langzeitsicherung und Archivierung digitaler Inhalte ist auf der Ebene der Bitstream-Archivierung gut untersucht und kann als gelöst angesehen werden; Probleme bestehen allerdings nach wie vor bei der Bewahrung der Ebene der so genannten information representation 
(nach OAIS).
Man kann dabei zwischen der Archivierung genuin digitaler Informationen und der Langzeitsicherung von Digitalisaten unterscheiden, die von vorhandenen analogen Objekten erstellt werden.
Hier können unterschiedliche Maßstabe an die Archivierung gelegt werden. 

Im Rahmen der Langzeitsicherung werden Dateien in stabilen, migrationsfähigen Formaten auf einem technisch wie organisatorisch sicheren Speichersystem gesichert.
Die Archivierung digitaler Daten setzt auf einem solchen Speicher auf, beinhaltet jedoch noch weitreichendere technische und organisatorische Festlegungen, die nicht nur eine physische Erhaltung der Daten, sondern auch Strategien zur Bereitstellung für die Nutzung (Access) der Daten, auch im Kontext vorhandener und zukünftiger Informationssysteme.
Die Speichersysteme müssen auf Redundanz ausgelegt sein. 

Für die Langzeitarchivierung sind besonders technische Informationen und solche über die Veränderungshistorie eines Objekts wichtig. [60] Insbesondere, wenn es um die Veränderungshistorie eines Objekts geht, hat sich PREMIS (Preservation Metadata: Implementation Strategies) [61] als Datenmodell etabliert. 

Ob die erzeugten digitalen Inhalte langzeitgesichert oder archiviert werden, hängt von der Strategie der jeweiligen Institution ab, die mit einer Antragstellung erläutert werden muss. [62] 
Wesentliche Erfolgsfaktoren für eine Langzeitsicherung und Archivierung von digitalen Dokumenten sind 

* (1) die Schaffung der organisatorischen und wirtschaftlichen Rahmenbedingungen, 
* (2) die Schaffung der technischen Rahmenbedingungen bzw. die Auswahl einer geeigneten technischen Methode/Strategie. 

Für die Archivierung digitaler Daten ist das Open Archival Information System (OAIS) als Referenzmodell anzuwenden. [63] 
Die „Kriterien für vertrauenswürdige digitale Langzeitarchive“ sind essenziell.
Mit ihnen werden u. a. der organisatorische Rahmen, gesetzlichen Rahmenbedingungen, das Qualitätsmanagement sowie die Authentizität für ein vertrauenswürdiges Archiv geschaffen. [64] 
Die Umsetzung der Langzeitsicherung erfolgt in „Digitalen Magazinen“ oder Reproduktionenverwaltungen, die sich am OAIS-Modell orientieren. 

Die Langzeitverfügbarkeit der Ergebnisse von Digitalisierungsprojekten ergibt sich einerseits aus der Wahl der Daten- und Metadatenformate. Überlegungen dazu flossen in die vorigen Kapitel ein.
Andererseits ist sicherzustellen, dass die digitalen Daten auch physikalisch verfügbar bleiben.
Dabei gilt: Kosten für die projektbezogene Sicherung der Daten werden in DFG-geförderten Digitalisierungsprojekten als Eigenleistung für die Laufzeit des Projekts anerkannt.
Eine Förderung dieser Kosten aus DFG-Mitteln kann nicht erfolgen. 

Die Langzeitsicherungs- bzw. Archivierungsfrage ist ein integraler Bestandteil jedes Digitalisierungsvorhabens.
Aufwand und Kosten sollten nicht unterschätzt werden.
Nicht nur die Kosten für den Speicherplatz, der je nach Projekt mehrere Terabyte betragen kann, sondern auch der Aufwand für die physische Erhaltung müssen langfristig berücksichtigt werden. 

Es sei darauf hingewiesen, dass Digitalisierungsprojekte aus Sicht der DFG stets Projekte der gesamten Einrichtung sind: Die Unterstützung der das Projekt abwickelnden Fachabteilung durch die IT-Infrastruktur des Hauses wird vorausgesetzt.
Dabei wird begrüßt, wenn sich kleinere Einrichtungen der Kompetenz und der Dienstleistung größerer Einrichtungen bedienen. 

Anträge müssen nachvollziehbare Aussagen zur institutionellen Langzeitsicherung und Archivierung enthalten. 



[60] http://www.langzeitarchivierung.de/Subsites/nestor/DE/Standardisierung/Metadaten.html;jsessionid=5239B697C2522475FBECD446877A114F.prod-worker5 
[61] http://www.loc.gov/standards/premis/understanding_premis_german.pdf 
[62] Eine Institution kann in ihrer Strategie z. B. schlüssig festlegen, dass born digital-Objekte archiviert werden und Abbilder von analogen Objekten auf langzeitsicheren Speichern abgelegt werden. 
Ebenso ist es aus guten Gründen möglich, auch die vom Original erstellten Digitalisate zu archivieren, wenn z. B. die Originale fragil sind. 
[63] Das OAIS- Referenzmodell ist als ISO 14721-Standard verabschiedet: http://public.ccsds.org/publications/archive/650x0m2.pdf  
[64] Vgl. Schoger, Astrid/Susanne Dobratz/Reinhard Altenhöner: Kriterienkatalog vertrauenswürdige digitale Langzeitarchive, Frankfurt am Main, 2008. Vgl.: http://nbn-resolving.de/urn:nbn:de:0008-2008021802. Vgl. dazu auch: DIN 31644:2012-04. 
